<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Addestramento Rete Neurale con Keras</title>
    <link rel="stylesheet" href="../../css/style.css">
    <link rel="icon" href="../../images/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="../../js/main.js"></script>

    <style>
        .btn {
            margin-right: 10px;
            margin-bottom: 10px;
            display: block;
            width: auto;
            padding: 8px 15px;
            text-align: center;
        }

        .btn-back, .btn-tutorial {
            width: auto;
            max-width: 350px;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div>
                    <h1>Addestramento con Keras</h1>
                    <p>Tutorial: Implementazione e addestramento di una MLP sul dataset MNIST</p>
                </div>
            </div>
        </div>
    </header>
    
    <div class="container">
        <a href="keras.html" class="btn btn-back">← Torna a Keras</a>
        
        <!-- CONTENUTO: Addestramento di una MLP su MNIST -->
        <div class="content-section">
            <h2>Addestramento di una Rete MLP su MNIST con Keras</h2>
            
            <p>In questa sezione, esaminiamo come implementare e addestrare una rete neurale multi-layer perceptron (MLP) classica sul dataset MNIST usando Keras. La semplicità e l'eleganza dell'API Keras sono evidenti in questo esempio.
                Questo offre numerosi vantaggi, sbagliare è più difficile usando codice già testato e scritto da esperti del settore. Testare diventa veloce e facile da debuggare.
                Una grande comodità di Keras sono le Callbacks, che permettono di monitorare e controllare l'addestramento in maniera automatica, cosa che non succede in JAX. Le Callbacks contornano 
                il core del codice, gli utilizzi classici sono il salvataggio dei pesi durante il training oppure lo stop dell'allenamento quando una condizione è soddisfatta (es: accuratezza maggiore di una soglia).

                Iniziamo con l'importare le librerie necessarie e configurare i parametri di base per l'addestramento. Utilizzeremo il dataset MNIST, un classico per la classificazione delle cifre scritte a mano, e costruiremo una rete MLP semplice ma efficace.
                Per installare le varie librerie si possono confrontare le relative documentazioni ufficiali. 
            </p>
            
            <h3>1. Importazione delle Librerie e Configurazione</h3>
            <div class="code-block">
<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt

# Iperparametri
BATCH_SIZE = 64
NUM_CLASSES = 10
NUM_EPOCHS = 10

# Impostazioni del formato per l'input
img_rows, img_cols = 28, 28
input_shape = (img_rows, img_cols, 1)</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>2. Caricamento e Preparazione del Dataset MNIST</h3>
            <p>Il dataset MNIST, a causa della sua popolarità, è già integrato in Keras, quindi possiamo caricarlo direttamente chiamando l'apposita funzione. 
                Successivamente, modifichiamo le dimensione dei tensori immagini in quanto i nostri layer si aspettano delle immagini che abbiano dimensioni (1, altezza, largezza, canale_colore); 
                poi normalizziamo in modo da avere valori tra 0 e 1.</p>
            <div class="code-block">
<pre><code class="language-python"># Caricamento del dataset integrato di Keras
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Reshaping dei dati per adattarli al formato dell'input
x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)

# Normalizzazione dei dati
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# Conversione delle etichette in categorie one-hot
y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)
y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>3. Definizione dell'Architettura della Rete</h3>
            <p> Più layer vanno a comporre l'architettura sequenziale del modello, Keras si preoccuperà di passare l'immagine in input ad ognuno di essi nell'oridine in cui li definiamo,
                e di passare l'output dell'ultimo layer al layer di output. La funzione Flatten serve a trasformare le immagini 28x28 in vettori 784, che sono i dati in input per il primo hidden layer. 
                I layer Dense sono i layer fully connected, mentre il Dropout serve a regolarizzare il modello per evitare overfitting. Infine, l'output layer ha una funzione di attivazione softmax per la classificazione multi-classe.
                La funzione di attivazione softmax è utile per ottenere probabilità normalizzate per ciascuna classe, in modo da poter interpretare l'output come una distribuzione di probabilità.</p>
                NOTA: tantissime informazioni sono addensate in queste poche righe di codice: l'attivazione avviene in maniera non esplicita nel grafo computazionale, 
                le dimensioni in input di ogni layer sono implicite dichiariamo solo le dimensioni di output. E se volessi vedere l'output latente di un solo layer durante l'allenamento per studiarlo? Da questo codice non 
                mi sembra di avere un modo facile ed intuitivo per poterlo fare.<div class="code-block">
<pre><code class="language-python"># Utilizzo dell'API Sequenziale di Keras
model = keras.Sequential([
    # Flatten dell'input (trasforma le immagini 28x28 in vettori 784)
    layers.Flatten(input_shape=input_shape),
    
    # Primo hidden layer
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.2),  # Regolarizzazione
    
    # Secondo hidden layer
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.2),  # Regolarizzazione
    
    # Output layer con softmax per la classificazione
    layers.Dense(NUM_CLASSES, activation='softmax')
])

# Riepilogo dell'architettura del modello
model.summary()</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <div class="output-block">
<pre><code class="language-output">Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 512)               401,920   
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 256)               131,328   
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2,570     
=================================================================
Total params: 535,818
Trainable params: 535,818
Non-trainable params: 0
_________________________________________________________________</code></pre>
            </div>
            
            <h3>4. Compilazione del Modello</h3>
            <p>La compilazione del modello è il passo finale prima di iniziare l'addestramento. Qui definiamo la funzione di loss, l'ottimizzatore e le metriche da monitorare durante l'addestramento.
                Queste sono le decisioni principali che dobbiamo prendere prima di iniziare l'addestramento e che rispecchiano il task che vogliamo risolvere.
                In questo caso, utilizziamo la funzione di loss 'categorical_crossentropy' per la classificazione multi-classe e l'ottimizzatore Adam, che è uno dei più popolari e performanti per il deep learning.</p>
                Attenzione, compilazione qui è intesa in maniera diversa rispetto a quella che si usa in JAX. Usare il metodo compile di Keras non significa che il codice venga compilato in un grafo computazionale, ma che viene creato un oggetto che contiene le informazioni necessarie per l'addestramento del modello.
                Keras farà delle ottimizzazione dietro le quinte, ma non nel senso stretto di creare del codice ottimizzato a basso livello per l'esecuzione, come fa JAX quando chiamiamo esplicitamente la compilazione just-in-time.
                </p>
            <div class="code-block">
<pre><code class="language-python"># Configurazione del processo di ottimizzazione
model.compile(
    loss='categorical_crossentropy',  # Funzione di loss per classificazione multi-classe
    optimizer=keras.optimizers.Adam(),  # Ottimizzatore Adam con parametri default
    metrics=['accuracy']  # Monitoriamo l'accuratezza durante l'addestramento
)</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>5. Definizione di Callback e Addestramento</h3>
            <div class="code-block">
<pre><code class="language-python"># Definizione di callback per monitorare e controllare l'addestramento
callbacks = [
    # Early stopping per interrompere l'addestramento quando non c'è miglioramento
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=3,
        verbose=1
    ),
    # Checkpoint per salvare il miglior modello
    keras.callbacks.ModelCheckpoint(
        'best_mnist_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    ),
    # Riduzione del learning rate quando l'apprendimento si blocca
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=2,
        verbose=1,
        min_lr=1e-6
    )
]

# Addestramento del modello
history = model.fit(
    x_train, y_train,
    batch_size=BATCH_SIZE,
    epochs=NUM_EPOCHS,
    verbose=1,
    validation_split=0.1,  # 10% dei dati di training usati per validazione
    callbacks=callbacks
)</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>6. Valutazione del Modello</h3>
            <div class="code-block">
<pre><code class="language-python"># Valutazione sul test set
score = model.evaluate(x_test, y_test, verbose=0)
print(f'Test loss: {score[0]:.4f}')
print(f'Test accuracy: {score[1]:.4f}')</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>7. Visualizzazione dei Risultati</h3>
            <div class="code-block">
<pre><code class="language-python"># Estrazione della storia dell'addestramento
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(1, len(acc) + 1)

# Creazione di un grafico con due subplot
plt.figure(figsize=(12, 5))

# Subplot per l'accuratezza
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

# Subplot per la loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.tight_layout()
plt.savefig('keras_mnist_history.png')
plt.show()</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>Output Tipico dell'Addestramento</h3>
            <div class="output-block">
<pre><code class="language-output">x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples

Epoch 1/10
844/844 [==============================] - 4s 4ms/step - loss: 0.2158 - accuracy: 0.9345 - val_loss: 0.0861 - val_accuracy: 0.9763
Epoch 2/10
844/844 [==============================] - 3s 3ms/step - loss: 0.0889 - accuracy: 0.9733 - val_loss: 0.0675 - val_accuracy: 0.9800
Epoch 3/10
844/844 [==============================] - 3s 3ms/step - loss: 0.0669 - accuracy: 0.9798 - val_loss: 0.0559 - val_accuracy: 0.9835
Epoch 4/10
844/844 [==============================] - 3s 3ms/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9843
Epoch 5/10
844/844 [==============================] - 3s 3ms/step - loss: 0.0451 - accuracy: 0.9859 - val_loss: 0.0492 - val_accuracy: 0.9852

Epoch 00005: early stopping
Epoch 00005: val_accuracy improved from 0.98430 to 0.98517, saving model to best_mnist_model.h5

Test loss: 0.0394
Test accuracy: 0.9876</code></pre>
            </div>
            
            <p>Questo esempio dimostra la semplicità e la chiarezza dell'API Keras. La maggior parte dello sforzo nello scrivere questo codice, infatti, proviene dall'organizzare e trasformare opportunamante
                i dati e dal plotting finale dei risultati, il resto della "pipeline" la gestisce Keras per noi: la creazione del modetto, l'utilizzo di una funzione di loss e di un ottimizzatore; ci è solo bastato inserire le nostre scelte nel metodo compile,
                 fit ed evaluate dell'oggetto model. Con poche righe di codice, siamo in grado di costruire, addestrare e valutare una rete neurale complessa. La struttura modulare e l'API ad alto livello rendono il processo intuitivo, pur mantenendo una buona potenza e flessibilità.</p>
        </div>
    </div>
    
    <footer>
        <div class="container">
            <p>© 2025 - Guida alle Librerie per Reti Neurali</p>
        </div>
    </footer>
</body>
</html>