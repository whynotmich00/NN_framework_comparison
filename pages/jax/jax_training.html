<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Addestramento Rete Neurale con JAX</title>
    <link rel="stylesheet" href="../../css/style.css">
    <link rel="icon" href="../../images/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="../../js/main.js"></script>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div>
                    <h1>Addestramento con JAX</h1>
                    <p>Tutorial: Implementazione e addestramento di una MLP sul dataset MNIST</p>
                </div>
            </div>
        </div>
    </header>
    
    <div class="container">
        <a href="jax.html" class="btn btn-back">← Torna a JAX</a>
        
        <div class="content-section">
            <h2>Addestramento di una Rete MLP su MNIST con JAX</h2>
            
            <p>In questa sezione, mostreremo come implementare e addestrare una rete neurale multi-layer perceptron (MLP) sul dataset MNIST usando solamente JAX.
                Questo vuol dire che non utilizzeremo Flax e Optax per layer e ottimizzatori, nella vita di tutti giorni è più comune utilizzare queste librerie.
                Per raggiungere l'obiettivo dovremo implemetare:
                <ul>
                    <li>Un modello: MLP con funzioni di attivatione ReLU</li>
                    <li>Un ottimizzatore: SGD</li>
                    <li>Una funzione di loss: Cross-Entropy</li>
                    <li>Una funzione che definisce il singolo step di training: calcolo del gradiente + aggiornamento pesi</li>
                    <li>Una ciclo di allenamento: per aggiornare i pesi del modello più volte (for loop)</li>
                    <li>Altre funzioni minori: loading dei dati, calcolo dell'accuratezza, inizializzazione dei pesi e illustrazione dei risultati </li>
                </ul>
            </p>
            
            <h3>1. Importazione delle Librerie e Configurazione</h3>
            <p>Iniziamo importando le librerie necessarie e configurando JAX per l'uso della GPU. Per installare JAX seguire gli step della documentazione ufficiale. 
                Se non si possiede una GPU, JAX funzionerà comunque in CPU, ma sarà più lento. Inoltre definiamo alcuni iperparametri (in maiuscolo) che useremo in seguito.</p>

            <div class="code-block">
<pre><code class="language-python">import jax
import jax.numpy as jnp
import jax.random as jrn
import jax.tree_util as jtu
from jax import grad, jit, vmap
import numpy as np
from functools import partial
import matplotlib.pyplot as plt

import tensorflow as tf
tf.config.set_visible_devices([], 'GPU') # Disabilita l'uso della GPU in TensorFlow

# Impostazione della precisione per output più leggibili
jax.config.update('jax_enable_x64', False)

# Verifica dispositivo disponibile
print("Dispositivi disponibili:", jax.devices())

# Iperparametri
BATCH_SIZE = 64
LEARNING_RATE = 0.001
NUM_EPOCHS = 10
</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>2. Caricamento e Preparazione del Dataset MNIST</h3>
            <p>Carichiamo il dataset MNIST, normalizziamo i dati e prepariamo le etichette. MNIST è un benchmark classico per la classificazione di immagini di cifre scritte a mano.

                Attualmente, JAX non dispone di un proprio sistema nativo per il caricamento dei dati. Per questo motivo, quando si lavora con JAX, è necessario affidarsi ai DataLoader di TensorFlow o PyTorch. In questo esempio, utilizziamo TensorFlow, generalmente preferito per questa operazione.
                
                Questa limitazione rappresenta un punto debole di JAX: non solo manca un sistema integrato per il data loading, ma sono anche assenti molte delle trasformazioni comuni nel preprocessing, in particolare per immagini e dati audio.
                Al di fuori degli esempi puramente didattici queste trasformazioni
                sono fondamentali per ottenere buone performance in termini di accuratezza e generalizzazione del modello.
                Quindi, in tal caso, dovremo appoggiarci a librerie come <a href="https://www.tensorflow.org/datasets?hl=it" >tensorflow_datasets</a> e <a href="https://pytorch.org/vision/stable/index.html" >torchvision</a>.</p>
                 </p>
            <div class="code-block">
<pre><code class="language-python">def load_dataset():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train = x_train.astype(np.float32) / 255.0
    x_test = x_test.astype(np.float32) / 255.0
    x_train = x_train.reshape(-1, 28*28)
    x_test = x_test.reshape(-1, 28*28)
    y_train = y_train.astype(np.int32)
    y_test = y_test.astype(np.int32)
    return x_train, y_train, x_test, y_test

x_train, y_train, x_test, y_test = load_dataset()
print("Dimensioni dei dati di training:", x_train.shape)
print("Dimensioni delle etichette di training:", y_train.shape)

def get_batch(rng, x, y, batch_size):
    dataset_size = x.shape[0]
    key, rng = random.split(rng)
    indices = random.permutation(key, jnp.arange(dataset_size))
    start_idx = 0
    while start_idx < dataset_size:
        batch_idx = indices[start_idx:start_idx + batch_size]
        yield x[batch_idx], y[batch_idx]
        start_idx += batch_size</code></pre>
        <button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>3. Definizione dell'Architettura della Rete</h3>
            <p>Di seguito definiamo delle funzioni particolarmente importanti, in questa sezione si possono mettere le mani in pasta e capire cosa accade quando si allena una rete in maniera abbastanza approfondita.
                Chiaramente andare ancora di più nel dettaglio richiederebbe vedere il codice sorgente dopo che le funzioni sono state compilate da JAX, ma questa ha un'importanza relativa visto
                che non siamo degli sviluppatori JAX, oppure potremmo vedere come funzionano la funzione grad che è quella che nasconde una grossa parte di calcoli. Questo sarebbe molto interessante, ed, in parte,
                è stato fatto <a href="jax_trasparenza_referenziale.html">qui</a>.
                
                Iniziamo con il definire le funzioni che servono per l'inizializzazione dei pesi e dei bias della rete. La funzione init_weights inizializza i pesi con una distribuzione normale (si veda l'articolo di Kaiming), mentre init_bias inizializza i bias a zero.
                Poi definiamo la funzione dense che rappresenta un layer denso (fully connected) e la funzione net che rappresenta l'intera rete neurale MLP; come si può vedere queste non sono altro che combinazioni lineari seguite da non linearità.
                Ora ci serve una funzione che ci permetta di creare i parametri della rete, in questo caso una MLP con 2 layer latenti. La funzione create_net_params accetta le dimensioni di input, hidden e output e restituisce i parametri inizializzati.
                Infine, definiamo la funzione sgd che aggiorna i pesi usando l'algoritmo di discesa del gradiente stocastico puro e semplice (SGD).
                La funzione sgd_update è una versione parzialmente valutata della funzione sgd, che ci permette di specificare il learning rate in anticipo e non doverlo passare come parametro in ogni step dell'allenamento.</p>
            
            </p>
            <div class="code-block">
<pre><code class="language-python">def init_weights(shape, key):
    """Inizializza i pesi della rete con distribuzione normale"""
    scale = shape[0] ** -0.5 # kaiming initialization
    return random.normal(key, shape) * scale

def init_bias(shape, key):
    """Inizializza i bias della rete a zero"""
    return jnp.zeros(shape)

def dense(params, x):
    """Layer denso (fully connected)"""
    w, b = params
    return jnp.dot(x, w) + b

def net(params, x):
    """Rete neurale MLP"""
    x = dense(params[0], x)
    x = jax.nn.relu(x)
    x = dense(params[1], x)
    return x

def create_net_params(input_dim, hidden_dim, output_dim, rng):
    """Crea i parametri per una rete MLP con 2 layer latenti"""
    keys = jrn.split(rng, num=2)
    params = []
    in_dims = (input_dim, hidden_dim)
    out_dims = (hidden_dim, output_dim)
    for in_dim, out_dim, key in zip(in_dims, out_dims, keys):
        w = init_weights((in_dim, out_dim), key)
        b = init_bias((out_dim,), key)
        params.append((w, b))
    return params

def sgd(learning_rate, params, grads):
    """Aggiorna i pesi usando SGD"""
    updates = jtu.tree_map(lambda g: -learning_rate * g, grads)
    new_params = jtu.tree_map(lambda p, u: p + u, params, updates)
    return new_params

sgd_update = partial(sgd, learning_rate=LEARNING_RATE) # funzione parzialmente "valutata" che ci permette di scegliere il learning_rate
                                                       # a priori e non passarlo come parametro in ogni step dell'allenamento
</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>4. Implementazione delle Funzioni di Loss e Accuracy</h3>
            Anche qui, continuiamo con il definire funzioni che ci serviranno nel training step, in particolare la funzione di loss e la funzione che calcola l'accuratezza.
            La funzione di loss è la cross-entropy, che è una misura della differenza tra le distribuzioni previste e quelle reali. La funzione compute_metrics calcola sia la loss che l'accuratezza del modello.


            <div class="code-block">
<pre><code class="language-python">def cross_entropy_loss(logits, labels):
    labels_one_hot = jax.nn.one_hot(labels, 10)
    return -jnp.mean(jnp.sum(labels_one_hot * jax.nn.log_softmax(logits), axis=-1))

def compute_metrics(logits, labels):
    loss = cross_entropy_loss(logits, labels)
    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
    return {'loss': loss, 'accuracy': accuracy}</code></pre>
    <button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>5. Definizione dei Loop di Addestramento e Valutazione</h3>
            <p>Passiamo ora all'ultimo step :), le funzioni di addestramento e valutazione. La funzione train_step esegue un singolo passo di addestramento, calcolando la loss e gli aggiornamenti dei pesi per un batch di dati (immagini, etichette).
                Questa funzione è centrale, al suo interno creiamo un ulteriore funzione di loss. Al suo interno calcoliamo i logits dati i parametri correnti, ossia le nostre predizioni e 
                successivamente calcoliamo la loss con la funzione creata prima cross_entropy_loss, questo artificio viene fatto perchè jax.grad (o equivalentemente jax.value_and_grad)
                differenzia una funzione (pura) rispetto al suo primo input (params) se avessimo differenziato con cross_entropy_loss avremmo ricevuto dei gradienti rispetto ai logits e non è quello che vogliamo.
                Ottenuti i gradienti, tocca ora all'update vero e proprio dei parametri che viene fatto seguendo la regola della discesa del gradiente stocastico (sgd_update). 
                Un altro tassello fondamentale è la decorazione di train_step ed eval_step con jax.jit, che compila le funzioni per l'esecuzione su GPU, rendendo il codice più veloce.
                Le funzioni che vengono compilare just-in-time sono soggette a diversi vincoli, uno tra questi è che in input non possono essere passati oggetti mutabili (es. liste) oppure altre funzioni (es. model, nel nostro caso)
                a causa di ciò bisogna dire a jit di non tracciare nel grafo computazionale la funzione model, ma solo la computazione che "passa per" batch e params, gli altri input.
                

                Da notare che solo la funzione più esterna deve essere decorata con jax.jit, e non ogni funzione interna.
                
                La funzione eval_step calcola le metriche di valutazione sul set di test seguendo la stessa logica di trian_step ma senza il calcolo del gradiente.</p>
            <div class="code-block">
<pre><code class="language-python">@partial(jax.jit, static_argnames=("model"))
def train_step(batch, params, model):
    x, y = batch
    def loss_fn(params):
        logits = model(params, x)
        loss = cross_entropy_loss(logits, y)
        return loss, logits
    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)
    new_params = sgd_update(params, grads)
    metrics = compute_metrics(logits, y)
    return new_params, metrics

@partial(jax.jit, static_argnames=("model"))
def eval_step(batch, params, model):
    x, y = batch
    logits = model(params, x)
    return compute_metrics(logits, y)</code></pre>
    <button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>6. Esecuzione dell'Addestramento</h3>
            Da qui non ci sono grosse differenze rispetto ad un allenamento che si osserva con altri framework tipo PyTorch, abbiamo il classico training loop, in cui per ogni epoca e per ogni
            batch di dati si chiama il train_step/eval_step e si calcolano le metriche di addestramento.

            <div class="code-block">
<pre><code class="language-python">def train_epoch(params, model, train_ds, batch_size, rng):
    train_metrics = []
    for x, y in get_batch(rng, *train_ds, batch_size):
        params, metrics = train_step((x, y), params, model)
        train_metrics.append(metrics)
    train_metrics = jnp.array(train_metrics).mean(axis=0)
    return params, train_metrics, rng

def eval_model(params, model, test_ds, batch_size):
    test_metrics = []
    for x, y in get_batch(random.PRNGKey(0), *test_ds, batch_size):
        metrics = eval_step((x, y), params, model)
        test_metrics.append(metrics)
    test_metrics = jnp.array(test_metrics).mean(axis=0)
    return test_metrics

rng = random.PRNGKey(0)
rng, init_rng = random.split(rng)
params = create_net_params(input_dim=28*28, hidden_dim=128, output_dim=10, rng=init_rng)
model = net         # sono d'accordo, questa linea è brutta (da modificare in futuro)
train_ds = (x_train, y_train)
test_ds = (x_test, y_test)
train_metrics_history = []
test_metrics_history = []

import time
start = time.time()
for epoch in range(NUM_EPOCHS):
    rng, epoch_rng = random.split(rng)
    params, train_metrics, rng = train_epoch(params, model, train_ds, BATCH_SIZE, epoch_rng)
    test_metrics = eval_model(params, model, test_ds, BATCH_SIZE)
    print(f'Epoca {epoch+1}/{NUM_EPOCHS}: '
          f'train_loss={train_metrics[0]:.4f}, '
          f'train_accuracy={train_metrics[1]:.4f}, '
          f'test_loss={test_metrics[0]:.4f}, '
          f'test_accuracy={test_metrics[1]:.4f}')
    train_metrics_history.append(train_metrics)
    test_metrics_history.append(test_metrics)
print(f"Durata di training: {time.time() - start}")</code></pre>
    <button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>

            <h3>Output Tipico dell'Addestramento</h3>
            <div class="output-block">
<pre><code class="language-none">Dispositivi disponibili: [CudaDevice(id=0)]
Dimensioni dei dati di training: (60000, 784)
Dimensioni delle etichette di training: (60000,)
Epoca 1/10: train_loss=2.1064, train_accuracy=0.3885, test_loss=1.8790, test_accuracy=0.6175
Epoca 2/10: train_loss=1.6746, train_accuracy=0.6960, test_loss=1.4477, test_accuracy=0.7553
Epoca 3/10: train_loss=1.2898, train_accuracy=0.7766, test_loss=1.1133, test_accuracy=0.8043
Epoca 4/10: train_loss=1.0207, train_accuracy=0.8093, test_loss=0.8987, test_accuracy=0.8294
Epoca 5/10: train_loss=0.8502, train_accuracy=0.8294, test_loss=0.7636, test_accuracy=0.8461
Epoca 6/10: train_loss=0.7397, train_accuracy=0.8420, test_loss=0.6742, test_accuracy=0.8566
Epoca 7/10: train_loss=0.6641, train_accuracy=0.8522, test_loss=0.6113, test_accuracy=0.8639
Epoca 8/10: train_loss=0.6096, train_accuracy=0.8595, test_loss=0.5647, test_accuracy=0.8708
Epoca 9/10: train_loss=0.5684, train_accuracy=0.8651, test_loss=0.5291, test_accuracy=0.8758
Epoca 10/10: train_loss=0.5360, train_accuracy=0.8701, test_loss=0.5008, test_accuracy=0.8799
Durata di training: 13.31 secondi
</code></pre>
            </div>

            <h3>7. Visualizzazione dei Risultati</h3>
            <div class="code-block">
<pre><code class="language-python">epochs = range(1, NUM_EPOCHS + 1)
train_loss = [loss for loss, _ in train_metrics_history]
train_accuracy = [accuracy for _, accuracy in train_metrics_history]
test_loss = [loss for loss, _ in test_metrics_history]
test_accuracy = [accuracy for _, accuracy in test_metrics_history]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, label='Train Loss', marker="o", alpha=0.7)
plt.plot(epochs, test_loss, label='Test Loss', marker="o", alpha=0.7)
plt.xlabel('Epoche')
plt.ylabel('Loss')
plt.title('Training e Test Loss')
plt.grid()
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracy, label='Train Accuracy', marker="o", alpha=0.7)
plt.plot(epochs, test_accuracy, label='Test Accuracy', marker="o", alpha=0.7)
plt.xlabel('Epoche')
plt.ylabel('Accuratezza')
plt.title('Training e Test Accuracy')
plt.grid()
plt.legend()

plt.tight_layout()
plt.savefig('jax_mnist_history.png')
plt.show()</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            <div class="output-jax-training">
                    <img src="../../images/jax_mnist_history.png" alt="Training accuracy jax" class="training-jax-image">
            </div>
            <p>Con questo esempio abbiamo voluto riportare un allenamento tipo utilizzando solamente le funzionanlità di JAX, questo ci ha permesso di capire quali sono le sue
                peculiarità da un punto di vista pratico, quali sono i principi da rispettare per fare in modo di sfruttare a pieno le sue potenzialità, ma soprattutto ci ha permesso
                di comprendere le sue differenze con altri framework in cui può essere più limitante personalizzare l'allenamento della propria rete neurale. Con JAX lo sviluppatore assume
                una maggiore responsabilità, un piccolo errore può facilmente compromettere l'allenamento, anche in maniera silenziosa, per fortuna jax.debug offre numerose soluzioni per scovare
                alcuni problemi classici come scovare dove si generano valori nan oppure disabilitare jit nel programma.
            </p>
        </div>
    </div>

    <footer>
        <div class="container">
            <p>© 2025 - Guida alle Librerie per Reti Neurali</p>
        </div>
    </footer>
</body>
</html>