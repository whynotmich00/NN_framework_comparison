<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Addestramento Rete Neurale con JAX</title>
    <link rel="stylesheet" href="../../css/style.css">
    <link rel="icon" href="../../images/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div>
                    <h1>Addestramento con JAX</h1>
                    <p>Tutorial: Implementazione e addestramento di una MLP sul dataset MNIST</p>
                </div>
            </div>
        </div>
    </header>
    
    <div class="container">
        <a href="jax.html" class="btn btn-back">← Torna a JAX</a>
        
        <div class="content-section">
            <h2>Addestramento di una Rete MLP su MNIST con JAX</h2>
            
            <p>In questa sezione, mostreremo come implementare e addestrare una rete neurale multi-layer perceptron (MLP) sul dataset MNIST usando solamente JAX.
                Questo vuol dire che non utilizzeremo Flax e Optax per layer e ottimizzatori, nella vita di tutti giorni è più comune utilizzare queste librerie.
                Per raggiungere l'obiettivo dovremo implemetare:
                <ul>
                    <li>Un modello: MLP con funzioni di attivatione ReLU</li>
                    <li>Un ottimizzatore: SGD</li>
                    <li>Una funzione di loss: Cross-Entropy</li>
                    <li>Una funzione che definisce il singolo step di training: calcolo del gradiente + aggiornamento pesi</li>
                    <li>Una ciclo di allenamento: per aggiornare i pesi del modello più volte (for loop)</li>
                    <li>Altre funzioni minori: loading dei dati, calcolo dell'accuratezza, inizializzazione dei pesi e illustrazione dei risultati </li>
                </ul>
            </p>
            
            <h3>1. Importazione delle Librerie e Configurazione</h3>
            <div class="code-block">
<pre><code class="language-python">import jax
import jax.numpy as jnp
import jax.random as jrn
import jax.tree_util as jtu
from jax import grad, jit, vmap
import numpy as np
from functools import partial
import matplotlib.pyplot as plt

# Impostazione della precisione per output più leggibili
jax.config.update('jax_enable_x64', False)

# Verifica dispositivo disponibile
print("Dispositivi disponibili:", jax.devices())

# Iperparametri
BATCH_SIZE = 64
LEARNING_RATE = 0.001
NUM_EPOCHS = 10
</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>2. Caricamento e Preparazione del Dataset MNIST</h3>
            <div class="code-block">
<pre><code class="language-python">import tensorflow as tf
tf.config.set_visible_devices([], 'GPU') # Disabilita l'uso della GPU in TensorFlow

def load_dataset():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train = x_train.astype(np.float32) / 255.0
    x_test = x_test.astype(np.float32) / 255.0
    x_train = x_train.reshape(-1, 28*28)
    x_test = x_test.reshape(-1, 28*28)
    y_train = y_train.astype(np.int32)
    y_test = y_test.astype(np.int32)
    return x_train, y_train, x_test, y_test

x_train, y_train, x_test, y_test = load_dataset()
print("Dimensioni dei dati di training:", x_train.shape)
print("Dimensioni delle etichette di training:", y_train.shape)

def get_batch(rng, x, y, batch_size):
    dataset_size = x.shape[0]
    key, rng = random.split(rng)
    indices = random.permutation(key, jnp.arange(dataset_size))
    start_idx = 0
    while start_idx < dataset_size:
        batch_idx = indices[start_idx:start_idx + batch_size]
        yield x[batch_idx], y[batch_idx]
        start_idx += batch_size</code></pre>
        <button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>3. Definizione dell'Architettura della Rete</h3>
            <div class="code-block">
<pre><code class="language-python">def init_weights(shape, key):
    """Inizializza i pesi della rete con distribuzione normale"""
    scale = shape[0] ** -0.5 # kaiming initialization
    return random.normal(key, shape) * scale

def init_bias(shape, key):
    """Inizializza i bias della rete a zero"""
    return jnp.zeros(shape)

def dense(params, x):
    """Layer denso (fully connected)"""
    w, b = params
    return jnp.dot(x, w) + b

def net(params, x):
    """Rete neurale MLP"""
    x = dense(params[0], x)
    x = jax.nn.relu(x)
    x = dense(params[1], x)
    return x

def create_net_params(input_dim, hidden_dim, output_dim, rng):
    """Crea i parametri per una rete MLP con 2 layer latenti"""
    keys = jrn.split(rng, num=2)
    params = []
    in_dims = (input_dim, hidden_dim)
    out_dims = (hidden_dim, output_dim)
    for in_dim, out_dim, key in zip(in_dims, out_dims, keys):
        w = init_weights((in_dim, out_dim), key)
        b = init_bias((out_dim,), key)
        params.append((w, b))
    return params

def sgd(learning_rate, params, grads):
    """Aggiorna i pesi usando SGD"""
    updates = jtu.tree_map(lambda g: -learning_rate * g, grads)
    new_params = jtu.tree_map(lambda p, u: p + u, params, updates)
    return new_params

sgd_update = partial(sgd, learning_rate=LEARNING_RATE) # funzione parzialmente "valutata" che ci permette di scegliere il learning_rate
                                                       # a priori e non passarlo come parametro in ogni step dell'allenamento
</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>4. Implementazione delle Funzioni di Loss e Accuracy</h3>
            <div class="code-block">
<pre><code class="language-python">def cross_entropy_loss(logits, labels):
    labels_one_hot = jax.nn.one_hot(labels, 10)
    return -jnp.mean(jnp.sum(labels_one_hot * jax.nn.log_softmax(logits), axis=-1))

def compute_metrics(logits, labels):
    loss = cross_entropy_loss(logits, labels)
    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
    return {'loss': loss, 'accuracy': accuracy}</code></pre>
    <button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>5. Definizione dei Loop di Addestramento e Valutazione</h3>
            <div class="code-block">
<pre><code class="language-python">@partial(jax.jit, static_argnames=("model"))
def train_step(batch, params, model):
    x, y = batch
    def loss_fn(params):
        logits = model(params, x)
        loss = cross_entropy_loss(logits, y)
        return loss, logits
    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)
    new_params = sgd_update(params, grads)
    metrics = compute_metrics(logits, y)
    return new_params, metrics

@partial(jax.jit, static_argnames=("model"))
def eval_step(batch, params, model):
    x, y = batch
    logits = model(params, x)
    return compute_metrics(logits, y)</code></pre>
    <button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            
            <h3>6. Esecuzione dell'Addestramento</h3>
            <div class="code-block">
<pre><code class="language-python">def train_epoch(params, model, train_ds, batch_size, rng):
    train_metrics = []
    for x, y in get_batch(rng, *train_ds, batch_size):
        params, metrics = train_step((x, y), params, model)
        train_metrics.append(metrics)
    train_metrics = jtu.tree_map(lambda x: jnp.mean(jnp.array(x)), train_metrics)
    return params, train_metrics, rng

def eval_model(params, model, test_ds, batch_size):
    test_metrics = []
    for x, y in get_batch(jrn.PRNGKey(0), *test_ds, batch_size):
        metrics = eval_step((x, y), params, model)
        test_metrics.append(metrics)
    test_metrics = jax.tree_map(lambda x: jnp.mean(jnp.array(x)), test_metrics)
    return test_metrics

rng = jrn.PRNGKey(0)
rng, init_rng = jrn.split(rng)
params = create_net_params(input_dim=28*28, hidden_dim=128, output_dim=10, rng=init_rng)
model = net         # sono d'accordo, questa linea è brutta (da modificare in futuro)
train_ds = (x_train, y_train)
test_ds = (x_test, y_test)
train_metrics_history = []
test_metrics_history = []

for epoch in range(num_epochs):
    rng, epoch_rng = jrn.split(rng)
    params, train_metrics, rng = train_epoch(params, model, train_ds, batch_size, epoch_rng)
    test_metrics = eval_model(state, test_ds, batch_size)
    print(f'Epoca {epoch+1}/{num_epochs}: '
          f'train_loss={train_metrics["loss"]:.4f}, '
          f'train_accuracy={train_metrics["accuracy"]:.4f}, '
          f'test_loss={test_metrics["loss"]:.4f}, '
          f'test_accuracy={test_metrics["accuracy"]:.4f}')
    train_metrics_history.append(train_metrics)
    test_metrics_history.append(test_metrics)</code></pre>
    <button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>

            <h3>Output Tipico dell'Addestramento</h3>
            <div class="output-block">
<pre><code class="language-none">Dispositivi disponibili: [CudaDevice(id=0)]
Dimensioni dei dati di training: (60000, 784)
Dimensioni delle etichette di training: (60000,)
Epoca 1/10: train_loss=2.1064, train_accuracy=0.3885, test_loss=1.8790, test_accuracy=0.6175
Epoca 2/10: train_loss=1.6746, train_accuracy=0.6960, test_loss=1.4477, test_accuracy=0.7553
Epoca 3/10: train_loss=1.2898, train_accuracy=0.7766, test_loss=1.1133, test_accuracy=0.8043
Epoca 4/10: train_loss=1.0207, train_accuracy=0.8093, test_loss=0.8987, test_accuracy=0.8294
Epoca 5/10: train_loss=0.8502, train_accuracy=0.8294, test_loss=0.7636, test_accuracy=0.8461
Epoca 6/10: train_loss=0.7397, train_accuracy=0.8420, test_loss=0.6742, test_accuracy=0.8566
Epoca 7/10: train_loss=0.6641, train_accuracy=0.8522, test_loss=0.6113, test_accuracy=0.8639
Epoca 8/10: train_loss=0.6096, train_accuracy=0.8595, test_loss=0.5647, test_accuracy=0.8708
Epoca 9/10: train_loss=0.5684, train_accuracy=0.8651, test_loss=0.5291, test_accuracy=0.8758
Epoca 10/10: train_loss=0.5360, train_accuracy=0.8701, test_loss=0.5008, test_accuracy=0.8799
Durata di training: 13.31 secondi
</code></pre>
            </div>

            <h3>7. Visualizzazione dei Risultati</h3>
            <div class="code-block">
<pre><code class="language-python">epochs = range(1, NUM_EPOCHS + 1)
train_loss = [metrics[0] for metrics in train_metrics_history]
train_accuracy = [metrics[1] for metrics in train_metrics_history]
test_loss = [metrics[0] for metrics in test_metrics_history]
test_accuracy = [metrics[1] for metrics in test_metrics_history]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, label='Train Loss', marker="o", alpha=0.7)
plt.plot(epochs, test_loss, label='Test Loss', marker="o", alpha=0.7)
plt.xlabel('Epoche')
plt.ylabel('Loss')
plt.title('Training e Test Loss')
plt.grid()
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracy, label='Train Accuracy', marker="o", alpha=0.7)
plt.plot(epochs, test_accuracy, label='Test Accuracy', marker="o", alpha=0.7)
plt.xlabel('Epoche')
plt.ylabel('Accuratezza')
plt.title('Training e Test Accuracy')
plt.grid()
plt.legend()

plt.tight_layout()
plt.savefig('jax_mnist_history.png')
plt.show()</code></pre>
<button class="copy-button" onclick="copyCode(this)">Copia</button>
            </div>
            <div class="output-jax-training">
                    <img src="../../images/jax_mnist_history.png" alt="Training accuracy jax" class="training-jax-image">
                    <p class="image-caption">Schema della struttura e navigazione del sito web</p>
            </div>
            <p>Questo esempio dimostra come JAX, insieme a Flax, offra un approccio potente e flessibile all'implementazione di reti neurali...</p>
        </div>
    </div>

    <script>
        function copyCode(button) {
            // Trova l'elemento pre più vicino al pulsante
            const pre = button.parentElement.querySelector('pre');
            const code = pre.textContent;
            
            // Crea un elemento textarea temporaneo
            const textarea = document.createElement('textarea');
            textarea.value = code;
            textarea.setAttribute('readonly', '');
            textarea.style.position = 'absolute';
            textarea.style.left = '-9999px';
            document.body.appendChild(textarea);
            
            // Seleziona e copia il testo
            textarea.select();
            document.execCommand('copy');
            
            // Rimuovi l'elemento temporaneo
            document.body.removeChild(textarea);
            
            // Feedback visivo
            button.textContent = 'Copiato!';
            button.classList.add('success');
            
            // Ripristina il testo del pulsante dopo 2 secondi
            setTimeout(() => {
                button.textContent = 'Copia';
                button.classList.remove('success');
            }, 2000);
        }
    </script>

    <footer>
        <div class="container">
            <p>© 2025 - Guida alle Librerie per Reti Neurali</p>
        </div>
    </footer>
</body>
</html>