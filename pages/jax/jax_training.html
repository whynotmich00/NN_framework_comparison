<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Addestramento Rete Neurale con JAX</title>
    <link rel="stylesheet" href="../../css/style.css">
    <link rel="icon" href="../../images/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div>
                    <h1>Addestramento con JAX</h1>
                    <p>Tutorial: Implementazione e addestramento di una MLP sul dataset MNIST</p>
                </div>
            </div>
        </div>
    </header>
    
    <div class="container">
        <a href="jax.html" class="btn btn-back">← Torna a JAX</a>
        
        <div class="content-section">
            <h2>Addestramento di una Rete MLP su MNIST con JAX</h2>
            
            <p>In questa sezione, mostreremo come implementare e addestrare una rete neurale multi-layer perceptron (MLP) sul dataset MNIST usando JAX e la sua libreria companion Flax, che fornisce componenti di più alto livello per il deep learning.</p>
            
            <h3>1. Importazione delle Librerie e Configurazione</h3>
            <div class="code-block">
<pre><code class="language-python">import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random
import numpy as np
import matplotlib.pyplot as plt

# Flax è una libreria di neural network costruita su JAX
import flax
import flax.linen as nn
from flax.training import train_state
import optax  # Libreria di ottimizzatori per JAX

# Impostazione della precisione per output più leggibili
jax.config.update('jax_enable_x64', False)

# Verifica dispositivo disponibile
print("Dispositivi disponibili:", jax.devices())

# Iperparametri
batch_size = 64
learning_rate = 0.001
num_epochs = 10
momentum = 0.9</code></pre>
            </div>
            
            <h3>2. Caricamento e Preparazione del Dataset MNIST</h3>
            <div class="code-block">
<pre><code class="language-python">import tensorflow as tf
tf.config.set_visible_devices([], 'GPU')

def load_dataset():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train = x_train.astype(np.float32) / 255.0
    x_test = x_test.astype(np.float32) / 255.0
    x_train = x_train.reshape(-1, 28*28)
    x_test = x_test.reshape(-1, 28*28)
    y_train = y_train.astype(np.int32)
    y_test = y_test.astype(np.int32)
    return x_train, y_train, x_test, y_test

x_train, y_train, x_test, y_test = load_dataset()
print("Forma dei dati di training:", x_train.shape)
print("Forma delle etichette di training:", y_train.shape)

def get_batch(rng, x, y, batch_size):
    dataset_size = x.shape[0]
    key, rng = random.split(rng)
    indices = random.permutation(key, jnp.arange(dataset_size))
    start_idx = 0
    while start_idx < dataset_size:
        batch_idx = indices[start_idx:start_idx + batch_size]
        yield x[batch_idx], y[batch_idx]
        start_idx += batch_size</code></pre>
            </div>
            
            <h3>3. Definizione dell'Architettura della Rete con Flax</h3>
            <div class="code-block">
<pre><code class="language-python">class MLP(nn.Module):
    @nn.compact
    def __call__(self, x, training=True):
        x = nn.Dense(features=512)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.2, deterministic=not training)(x)
        x = nn.Dense(features=256)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.2, deterministic=not training)(x)
        x = nn.Dense(features=10)(x)
        return x

def create_train_state(rng, learning_rate, momentum):
    model = MLP()
    dummy_input = jnp.ones((1, 28*28))
    params = model.init(rng, dummy_input)['params']
    tx = optax.sgd(learning_rate=learning_rate, momentum=momentum)
    return train_state.TrainState.create(
        apply_fn=model.apply,
        params=params,
        tx=tx,
    )</code></pre>
            </div>
            
            <h3>4. Implementazione delle Funzioni di Loss e Accuracy</h3>
            <div class="code-block">
<pre><code class="language-python">def cross_entropy_loss(logits, labels):
    labels_onehot = jax.nn.one_hot(labels, 10)
    return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()

def compute_metrics(logits, labels):
    loss = cross_entropy_loss(logits, labels)
    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
    return {'loss': loss, 'accuracy': accuracy}</code></pre>
            </div>
            
            <h3>5. Definizione dei Loop di Addestramento e Valutazione</h3>
            <div class="code-block">
<pre><code class="language-python">@jax.jit
def train_step(state, batch):
    x, y = batch
    def loss_fn(params):
        logits = state.apply_fn({'params': params}, x, training=True)
        loss = cross_entropy_loss(logits, y)
        return loss, logits
    (loss, logits), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)
    state = state.apply_gradients(grads=grads)
    metrics = compute_metrics(logits, y)
    return state, metrics

@jax.jit
def eval_step(state, batch):
    x, y = batch
    logits = state.apply_fn({'params': state.params}, x, training=False)
    return compute_metrics(logits, y)</code></pre>
            </div>
            
            <h3>6. Esecuzione dell'Addestramento</h3>
            <div class="code-block">
<pre><code class="language-python">def train_epoch(state, train_ds, batch_size, rng):
    train_metrics = []
    for x, y in get_batch(rng, *train_ds, batch_size):
        state, metrics = train_step(state, (x, y))
        train_metrics.append(metrics)
    train_metrics = jax.tree_map(lambda x: jnp.mean(jnp.array(x)), train_metrics)
    return state, train_metrics, rng

def eval_model(state, test_ds, batch_size):
    test_metrics = []
    for x, y in get_batch(random.PRNGKey(0), *test_ds, batch_size):
        metrics = eval_step(state, (x, y))
        test_metrics.append(metrics)
    test_metrics = jax.tree_map(lambda x: jnp.mean(jnp.array(x)), test_metrics)
    return test_metrics

rng = random.PRNGKey(0)
rng, init_rng = random.split(rng)
state = create_train_state(init_rng, learning_rate, momentum)
train_ds = (x_train, y_train)
test_ds = (x_test, y_test)
train_metrics_history = []
test_metrics_history = []

for epoch in range(num_epochs):
    rng, epoch_rng = random.split(rng)
    state, train_metrics, rng = train_epoch(state, train_ds, batch_size, epoch_rng)
    test_metrics = eval_model(state, test_ds, batch_size)
    print(f'Epoca {epoch+1}/{num_epochs}: '
          f'train_loss={train_metrics["loss"]:.4f}, '
          f'train_accuracy={train_metrics["accuracy"]:.4f}, '
          f'test_loss={test_metrics["loss"]:.4f}, '
          f'test_accuracy={test_metrics["accuracy"]:.4f}')
    train_metrics_history.append(train_metrics)
    test_metrics_history.append(test_metrics)</code></pre>
            </div>

            <h3>Output Tipico dell'Addestramento</h3>
            <div class="output-block">
<pre><code class="language-none">Dispositivi disponibili: [CpuDevice(id=0)]
Forma dei dati di training: (60000, 784)
Forma delle etichette di training: (60000,)
Epoca 1/10: train_loss=0.3219, train_accuracy=0.9078, test_loss=0.1532, test_accuracy=0.9556
Epoca 2/10: train_loss=0.1488, train_accuracy=0.9561, test_loss=0.1084, test_accuracy=0.9674
...</code></pre>
            </div>

            <h3>7. Visualizzazione dei Risultati</h3>
            <div class="code-block">
<pre><code class="language-python">epochs = range(1, num_epochs + 1)
train_loss = [metrics['loss'] for metrics in train_metrics_history]
train_accuracy = [metrics['accuracy'] for metrics in train_metrics_history]
test_loss = [metrics['loss'] for metrics in test_metrics_history]
test_accuracy = [metrics['accuracy'] for metrics in test_metrics_history]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, label='Train Loss')
plt.plot(epochs, test_loss, label='Test Loss')
plt.xlabel('Epoche')
plt.ylabel('Loss')
plt.title('Training e Test Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracy, label='Train Accuracy')
plt.plot(epochs, test_accuracy, label='Test Accuracy')
plt.xlabel('Epoche')
plt.ylabel('Accuratezza')
plt.title('Training e Test Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('jax_mnist_history.png')
plt.show()</code></pre>
            </div>

            <p>Questo esempio dimostra come JAX, insieme a Flax, offra un approccio potente e flessibile all'implementazione di reti neurali...</p>
        </div>
    </div>

    <footer>
        <div class="container">
            <p>© 2025 - Guida alle Librerie per Reti Neurali</p>
        </div>
    </footer>
</body>
</html>