<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JAX - Libreria per Reti Neurali</title>
    <link rel="stylesheet" href="../../css/style.css">
    <link rel="icon" href="../../images/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div>
                    <h1>JAX</h1>
                    <p>Framework per differenziazione automatica e calcolo ad alte prestazioni</p>
                </div>
            </div>
        </div>
    </header>
    
    <div class="container">
        <a href="../../index.html" class="btn btn-back">← Torna alla Home</a>
        <a href="jax_fundamentals.html" class="btn btn-tutorial">VMAP, GRAD, JIT: Le tre colonne portanti di JAX →</a>
        <a href="jax_training.html" class="btn btn-tutorial">Tutorial: Addestramento MLP su MNIST →</a>
        <a href="jax_advanced.html" class="btn btn-tutorial">Avanzato: Differenziazione Automatica e Allenamento Distribuito →</a>
        
        <!-- SEZIONE 1: Introduzione, Backend e Architettura di JAX -->
        <div class="content-section">
            <h2>Introduzione a JAX (Numpy on steroids)</h2>
            <p><a href="https://github.com/google/jax" target="_blank">JAX</a> è un framework di calcolo numerico sviluppato da Google Research che combina <a href="https://numpy.org/" target="_blank">NumPy</a> con la <a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank">differenziazione automatica</a> e l'accelerazione hardware. La sua familiarità con la sintassi di NumPy
                lo rende comprensibile e facile da usare rendendo il percorso di apprendimento più semplice per chi si è già approcciato al calcolo numerico in Python. Quello che gli sviluppatori hanno fatto è stato prendere il
                codice sorgente aperto di NumPy e scriverne una versione apparentemente identica, ma che permette di essere eseguita su vari acceleratori come <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" target="_blank">GPU</a> e simili (vedi <a href="https://jax.readthedocs.io/en/latest/jax.numpy.html" target="_blank">jax.numpy</a>).
                Anche se spesso ci si trova a lavorare con JAX in modo simile a NumPy, JAX è molto più di una semplice libreria NumPy accelerata. Infatti, JAX offre anche la differenziazione automatica (<a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad" target="_blank">jax.grad</a>) e la compilazione just-in-time (<a href="https://jax.readthedocs.io/en/latest/jax.html#jax.jit" target="_blank">jax.jit</a>) per ottimizzare le prestazioni del codice.
                Queste funzioni vengono esplorate nel dettaglio nella pagina a loro dedicata "VMAP, GRAD, JIT: Le tre colonne portanti di JAX". Le potenzialità di JAX non si fermano qui, nella sua libreria possiamo trovare numerse altre feature che, per esempio, ci permettono di scrivere
                kernel per GPU e TPU senza l'utilizzo di <a href="https://developer.nvidia.com/cuda-toolkit" target="_blank">CUDA</a> o <a href="https://www.khronos.org/opencl/" target="_blank">OpenCL</a> (<a href="https://jax.readthedocs.io/en/latest/pallas/index.html" target="_blank">jax.pallas</a>) o parallelizzare il calcolo su più dispositivi modificando di poco il codice che si scriverebbe normalmente.
                Oltre NumPy anche <a href="https://docs.jax.dev/en/latest/jax.scipy.html" target="_blank">SciPy</a> è stata inglobata in JAX, permettendo di utilizzare molte funzioni avanzate di calcolo scientifico e statistico in maniera accelerata.
            </p>
        </div>

        <div class="content-section">
            <h1>La filosofia di JAX</h1>
            
            <p>JAX nasce con una filosofia profondamente radicata nella <span class="highlight">programmazione funzionale</span>, distinguendosi nettamente dall'approccio imperativo di framework come PyTorch.</p>
            
            <h2>Programmazione puramente funzionale</h2>
            <ul>
                <li>JAX è costruito attorno al concetto di <strong>funzioni pure</strong>: funzioni che, dato lo stesso input, producono sempre lo stesso output senza effetti collaterali</li>
                <li>Evita mutazioni di stato: le operazioni restituiscono nuovi array invece di modificare quelli esistenti</li>
                <li>Tutti gli array JAX sono immutabili (non modificabili)</li>
            </ul>
            
            <div class="example">
                <p>In NumPy (imperativo):</p>
                <pre><code>import numpy as np
    array = np.zeros(10)
    array[0] = 1  # Modifica l'array esistente</code></pre>
                
                <p>In JAX (funzionale):</p>
                <pre><code>import jax.numpy as jnp
    array = jnp.zeros(10)
    # Non è possibile fare array[0] = 1
    # Invece, creiamo un nuovo array:
    array = array.at[0].set(1)</code></pre>
            </div>
            
            <h2>Trasformazioni di funzioni</h2>
            <p>JAX introduce il concetto di "trasformazioni di funzioni di ordine superiore":</p>
            <ul>
                <li><code>jit</code>: compila funzioni con XLA per esecuzione accelerata</li>
                <li><code>grad</code>: calcola automaticamente gradienti</li>
                <li><code>vmap</code>: vettorizza funzioni su batch</li>
                <li><code>pmap</code>: parallelizza funzioni su più dispositivi</li>
            </ul>
            
            <p>Queste trasformazioni possono essere composte in modo elegante:</p>
            <pre><code>import jax
    import jax.numpy as jnp
    
    def loss_fn(params, x, y):
        y_pred = model(params, x)
        return jnp.mean((y_pred - y)**2)
    
    # Compone trasformazioni in modo semplice
    training_step = jax.jit(jax.grad(loss_fn))</code></pre>
            
            <h2>Composizione e trasparenza referenziale</h2>
            <ul>
                <li>Favorisce la composizione di funzioni più piccole per costruire sistemi complessi</li>
                <li>La trasparenza referenziale garantisce che le funzioni possano essere sostituite con i loro valori</li>
                <li>Incoraggia uno stile di programmazione più matematico e dichiarativo</li>
            </ul>
            
            <div class="example">
                <pre><code>def double(x):
        return x * 2
    
    def add_one(x):
        return x + 1
    
    # Composizione di funzioni
    transform = jax.jit(jax.vmap(lambda x: add_one(double(x))))
    
    # Applicazione a un batch
    batch = jnp.array([1, 2, 3, 4])
    result = transform(batch)  # [3, 5, 7, 9]</code></pre>
            </div>
            
            <h2>Approccio "NumPy su steroidi"</h2>
            <ul>
                <li>API familiare simile a NumPy, ma con accelerazione hardware e differenziazione automatica</li>
                <li><code>jax.numpy</code> è un sostituto quasi diretto di NumPy, ma con semantica funzionale</li>
            </ul>
            
            <h2>Principio della separazione tra dati e computazione</h2>
            <ul>
                <li>I modelli in JAX sono rappresentati come funzioni pure che operano su parametri</li>
                <li>I parametri sono passati esplicitamente, non incapsulati in oggetti come in PyTorch</li>
                <li>Favorisce l'architettura "dati fuori, dati dentro" rispetto all'approccio ad oggetti</li>
            </ul>
            
            <div class="example">
                <p>In PyTorch (orientato agli oggetti):</p>
                <pre><code>import torch
    
    class Model(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.layer = torch.nn.Linear(10, 1)
        
        def forward(self, x):
            return self.layer(x)
    
    model = Model()
    output = model(input_data)</code></pre>
                
                <p>In JAX (funzionale):</p>
                <pre><code>import jax
    import jax.numpy as jnp
    from jax import random
    
    def init_params(key):
        key, subkey = random.split(key)
        W = random.normal(subkey, (10, 1))
        b = jnp.zeros(1)
        return {"weights": W, "bias": b}
    
    def model(params, x):
        return jnp.dot(x, params["weights"]) + params["bias"]
    
    key = random.PRNGKey(42)
    params = init_params(key)
    output = model(params, input_data)</code></pre>

        <div class="content-section">
            <h2>Il Backend di JAX e la sua Architettura</h2>
            
            <p>Il nome "JAX" sta per "Just After eXecution" (subito dopo l'esecuzione), un riferimento al suo approccio di tracciamento e compilazione del codice.</p>
            
            <h3>Architettura e Componenti di JAX</h3>
            <p>JAX è costruito su una serie di componenti che lavorano insieme per fornire le sue funzionalità principali:</p>
            
            <ol>
                <li><strong><a href="https://www.tensorflow.org/xla" target="_blank">XLA (Accelerated Linear Algebra)</a></strong>: Un compilatore e runtime specializzato per algebra lineare accelerata, sviluppato inizialmente per TensorFlow. XLA ottimizza i calcoli per esecuzione su hardware specifico (CPU, <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" target="_blank">GPU</a>, <a href="https://cloud.google.com/tpu" target="_blank">TPU</a>).</li>
                
                <li><strong>Tracciamento e trasformazione di funzioni</strong>: JAX trasforma funzioni Python in rappresentazioni che possono essere analizzate, ottimizzate e compilate.</li>
                
                <li><strong>Differenziazione automatica</strong>: JAX implementa sia la differenziazione in avanti che all'indietro, permettendo di calcolare gradienti esatti di funzioni.</li>
                
                <li><strong>Interfaccia NumPy</strong>: JAX fornisce un'API familiare che imita NumPy, rendendo più facile la transizione per chi già conosce questa libreria.</li>
            </ol>
            
            <h3>Come Funziona JAX "Under the Hood"</h3>
            <p>JAX opera attraverso un processo multifase che permette di ottimizzare e accelerare il codice Python:</p>
            
            <h4>1. Tracciamento e Rappresentazione Funzionale</h4>
            <p>Quando si applica una trasformazione JAX (come <code><a href="https://jax.readthedocs.io/en/latest/jax.html#jax.jit" target="_blank">jit</a></code>, <code><a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad" target="_blank">grad</a></code>, o <code><a href="https://jax.readthedocs.io/en/latest/jax.html#jax.vmap" target="_blank">vmap</a></code>) a una funzione, JAX esegue queste operazioni:</p>
            <ul>
                <li>Traccia la funzione con valori concreti per capire quali operazioni vengono eseguite</li>
                <li>Costruisce un grafo computazionale chiamato "jaxpr" (JAX expression)</li>
                <li>Questa rappresentazione cattura l'essenza matematica della funzione, indipendentemente dagli aspetti specifici di Python</li>
            </ul>
            
            <h4>2. Trasformazione</h4>
            <p>Il jaxpr può essere trasformato in vari modi:</p>
            <ul>
                <li><strong><a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad" target="_blank">grad</a></strong>: Calcola i gradienti di una funzione rispetto ai suoi input</li>
                <li><strong><a href="https://jax.readthedocs.io/en/latest/jax.html#jax.vmap" target="_blank">vmap</a></strong>: Vettorizza una funzione lungo una dimensione aggiuntiva</li>
                <li><strong><a href="https://jax.readthedocs.io/en/latest/jax.html#jax.jit" target="_blank">jit</a></strong>: Compila una funzione per esecuzione ottimizzata</li>
                <li><strong><a href="https://docs.jax.dev/en/latest/_autosummary/jax.experimental.shard_map.shard_map.html#jax.experimental.shard_map.shard_map" target="_blank">shard_map</a></strong>: Parallelizzazione manuale di una funzione su dispositivi multipli</li>
            </ul>
            
            <h4>3. Compilazione con XLA</h4>
            <p>Il jaxpr trasformato viene compilato tramite <a href="https://www.tensorflow.org/xla" target="_blank">XLA</a>:</p>
            <ul>
                <li>XLA analizza il grafo computazionale e applica ottimizzazioni come fusione delle operazioni e riallocazione della memoria</li>
                <li>Genera codice specifico per l'hardware di destinazione (CPU, GPU, o TPU)</li>
                <li>Il codice compilato viene messo in cache per riutilizzarlo con input di forma simile</li>
            </ul>
            
            <h4>4. Esecuzione</h4>
            <p>Quando la funzione viene chiamata con input concreti:</p>
            <ul>
                <li>JAX utilizza il codice compilato se disponibile in cache</li>
                <li>In caso contrario, traccia e compila nuovamente</li>
                <li>I risultati vengono restituiti come array JAX (simili a ndarray di NumPy, ma immutabili)</li>
            </ul>
            
            <h3>Principi Funzionali e Immutabilità</h3>
            <p>JAX adotta una filosofia di programmazione funzionale che ha profondi effetti sul suo design:</p>
            <ul>
                <li><strong>Immutabilità</strong>: Gli array JAX sono immutabili, il che significa che le operazioni creano nuovi array invece di modificare quelli esistenti. Spesso ci capita di vedere 
                in Python delle modifiche dette in-place, questo raramente succede in JAX che nasce con l'obiettivo di imitare i linguaggi con logiche di <a href="https://it.wikipedia.org/wiki/Programmazione_funzionale" target="_blank">programmazione funzionale</a>,
                che a loro volta nascondo per portare il linguaggio formale matematico nei computer.
                Per esempio, la funzione matematica x**2 resituirà dato in input il valore 2 sempre il valore 4, non si scappa. Questa funzione non ha stati interni che cambiano ogni volta
                che la si chiama e che non hanno a che fare con la mera computazione della funzione. Si pensi invece ad un semplice for loop e ad una funzione che quando viene chiamata accetta in input i ed x 
                e poi calcola x**2, ecco che la funzione non è più pura perchè ha uno stato interno, per una stessa x abbiamo uno stesso output ma può essere chiamata con diversi i e nessuno se ne accorge.
                Questo banale esempio può essere esteso in diversi modi, pensiamo per esempio a delle chiamate ad API esterne ed ad una funzione che osserva queste chiamate, capita spesso che le chiamate restituiscano dati
                che hanno struttura diversa e non uniforme. Ma perchè insistere sulle funzioni pure? La prima motivazione è che una rete neural vuole approssimare una funzione matematica, tramite composizione di altre funzioni matematiche, quindi questa
                logica sembrerebbe prestarsi bene al nostro caso applicativo. La seconda motivazione (che mi è parso di capire) è che da un punto di vista implementativo e di velocità del programma, sapere che la mia funzione
                vedra nel corso della sua vita solo array lunghi n e mai di altre dimensioni è un grosso vantaggio, potrei infatti pensare di compilare questa funzione e usare ogni volta la funzione velocissima compilata per fare i
                miei calcoli, se invece il mio input cambia spesso la mia funzione vedrà ogni tanto delle matrici, ogni tanto degli array ed ogni tanto dei tensori di più dimensioni, a quel punto non è possibile
                sfruttare le potenzialità che offre una compilazione a basso livello, la stessa cosa vale per i tipo che caratterizza i dati in input.</li>
                <li><strong>Funzioni pure</strong>: JAX funziona meglio con funzioni pure (senza effetti collaterali), facilitando l'ottimizzazione e la parallelizzazione</li>
                <li><strong>Tracciabilità</strong>: Le funzioni devono essere tracciabili da JAX, il che pone alcune restrizioni sul codice Python che può essere usato</li>
            </ul>
            
            <p>Questa architettura dà a JAX diverse caratteristiche uniche rispetto ad altri framework di deep learning. La sua capacità di trasformare le funzioni in modo componibile (applicando più trasformazioni in sequenza) e la sua integrazione con l'ecosistema NumPy lo rendono particolarmente adatto per la ricerca e per compiti che richiedono alta prestazione computazionale.</p>
            
            
            
            <p>PIU NEL DETTAGLIO: Immagina di avere una funzione Python che fa dei calcoli matematici usando array NumPy (o meglio, <a href="https://jax.readthedocs.io/en/latest/jax.numpy.html" target="_blank">jax.numpy</a>). Quando chiedi a JAX di fare qualcosa di "speciale" con questa funzione (come compilarla per la velocità con <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.jit" target="_blank">jax.jit</a> o calcolarne il gradiente con <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad" target="_blank">jax.grad</a>), JAX non esegue immediatamente la tua funzione Python nel modo tradizionale.
                Invece, JAX fa il tracing (tracciamento) della funzione:
                Esecuzione Simbolica: JAX esegue la tua funzione una volta, ma invece di usare i valori numerici effettivi degli input, usa degli oggetti speciali chiamati tracer. Questi tracer sono come dei segnaposto che registrano le informazioni sulla forma (shape) e sul tipo di dati (dtype) degli array, ma non il loro valore concreto.
                Registrazione delle Operazioni: Man mano che la funzione viene eseguita con questi tracer, JAX non esegue le operazioni matematiche vere e proprie. Invece, registra la sequenza di operazioni fondamentali di JAX (chiamate primitive) che vengono applicate ai tracer. Ad esempio, se la tua funzione fa y = x + 1, JAX registra un'operazione di "addizione".
                Creazione di una Rappresentazione Intermedia (Jaxpr): Il risultato di questo processo di registrazione è una rappresentazione intermedia della tua funzione, chiamata jaxpr (JAX Program Representation). La jaxpr è una descrizione pura e funzionale del calcolo, indipendente dal codice Python originale e dai suoi dettagli implementativi (come i cicli for di Python, che vengono "srotolati" durante il tracing). È essenzialmente un grafo computazionale statico.
                </p>

        </div>

        <!-- SEZIONE: Ecosystem di JAX -->
        <div class="content-section" id="jax-ecosystem">
            <h2>L'Ecosistema di JAX</h2>
            <p>JAX è uno strumento potente che fornisce al programmatore numerosi mattoncini con cui poter costruire e sperimentare nuove idee.
                Dover costruire un'ottimizzatore o un layer o una funzione di loss possono presentare numerosi peculiarità tecniche non banali (vedi: overflow nella funzione softmax) che possono
                rallentare di molto gli esperimenti di un ricercatore (non siamo tutti dei programmatori esperti), per questo JAX è considerato un framework
                più complicato di altri. Per sopperire a questa difficoltà la community ha generato un fiorente ecosistema di librerie che estendono le sue funzionalità di base per applicazioni comuni e/o specifiche.
                Alcune delle librerie più importanti costruite su JAX:</p>
            
            <div class="library-cards">
                <a href="https://flax.readthedocs.io/" class="library-card" target="_blank">
                    <div class="card-image">
                        <img src="../../images/flax-logo.png" alt="Flax Logo" onerror="this.src='../../images/placeholder.png'">
                    </div>
                    <div class="card-content">
                        <h3>Flax</h3>
                        <p>Framework flessibile per l'addestramento di reti neurali in JAX. Fornisce astrazioni di alto livello per la definizione di modelli e l'implementazione di loop di addestramento mantenendo la flessibilità di JAX.</p>
                    </div>
                </a>
                
                <a href="https://optax.readthedocs.io/" class="library-card" target="_blank">
                    <div class="card-image">
                        <img src="../../images/optax-logo.png" alt="Optax Logo" onerror="this.src='../../images/placeholder.png'">
                    </div>
                    <div class="card-content">
                        <h3>Optax</h3>
                        <p>Libreria per l'ottimizzazione in JAX che fornisce implementazioni di algoritmi di ottimizzazione come SGD, Adam, e RMSProp. Supporta la composizione di trasformazioni del gradiente per costruire ottimizzatori personalizzati.</p>
                    </div>
                </a>
                
                <a href="https://blackjax-devs.github.io/blackjax/" class="library-card" target="_blank">
                    <div class="card-image">
                        <img src="../../images/blackjax-logo.png" alt="BlackJAX Logo" onerror="this.src='../../images/placeholder.png'">
                    </div>
                    <div class="card-content">
                        <h3>BlackJAX</h3>
                        <p>Libreria per l'inferenza bayesiana in JAX. Implementa metodi di campionamento MCMC (Markov Chain Monte Carlo) avanzati per applicazioni di statistica bayesiana e apprendimento automatico probabilistico.</p>
                    </div>
                </a>
            </div>
        </div>
    </div>
    
    <footer>
        <div class="container">
            <p>© 2025 - Guida alle Librerie per Reti Neurali</p>
        </div>
    </footer>

    <style>
    /* Stili per la sezione dell'ecosistema */
    .library-cards {
        display: flex;
        flex-wrap: wrap;
        gap: 20px;
        margin-top: 20px;
        justify-content: space-between;
    }
    
    .library-card {
        background-color: #ffffff;
        border-radius: 8px;
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        overflow: hidden;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
        width: calc(33.333% - 14px);
        min-width: 280px;
        text-decoration: none;
        color: inherit;
        display: flex;
        flex-direction: column;
    }
    
    .library-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 8px 20px rgba(0, 0, 0, 0.15);
    }
    
    .card-image {
        height: 160px;
        overflow: hidden;
        display: flex;
        align-items: center;
        justify-content: center;
        background-color: #f8f9fa;
    }
    
    .card-image img {
        max-width: 80%;
        max-height: 80%;
        object-fit: contain;
    }
    
    .card-content {
        padding: 16px;
        flex-grow: 1;
        display: flex;
        flex-direction: column;
    }
    
    .card-content h3 {
        margin-top: 0;
        margin-bottom: 10px;
        color: #1a73e8;
        font-size: 1.4em;
    }
    
    .card-content p {
        margin-bottom: 0;
        font-size: 0.95em;
        line-height: 1.5;
    }
    
    /* Responsive design */
    @media (max-width: 900px) {
        .library-card {
            width: calc(50% - 10px);
        }
    }
    
    @media (max-width: 600px) {
        .library-cards {
            flex-direction: column;
        }
        
        .library-card {
            width: 100%;
        }
    }
    </style>
</body>
</html>