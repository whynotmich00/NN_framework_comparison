<!DOCTYPE html>
<html lang="it">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analisi Prestazioni Framework Reti Neurali</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
</head>
<body>
    <header>
        <div class="container">
            <h1>Analisi Prestazioni Framework Reti Neurali</h1>
            <p>Confronto dettagliato delle prestazioni di Keras, PyTorch e JAX</p>
        </div>
    </header>
    
    <div class="container">
        <a href="index.html" class="back-link">← Torna alla Home</a>
        
        <div class="performance-section">
            <h2>Tabella Riassuntiva delle Prestazioni</h2>
            <table>
                <thead>
                    <tr>
                        <th>Parametro</th>
                        <th>Keras</th>
                        <th>PyTorch</th>
                        <th>JAX</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Velocità di Training (GPU)</td>
                        <td>Buona</td>
                        <td>Molto buona</td>
                        <td>Eccellente</td>
                    </tr>
                    <tr>
                        <td>Velocità di Inferenza</td>
                        <td>Moderata</td>
                        <td>Buona</td>
                        <td>Ottima</td>
                    </tr>
                    <tr>
                        <td>Efficienza memoria</td>
                        <td>Moderata</td>
                        <td>Buona</td>
                        <td>Eccellente</td>
                    </tr>
                    <tr>
                        <td>Tempo compilazione</td>
                        <td>Rapido</td>
                        <td>Immediato</td>
                        <td>Lento (JIT)</td>
                    </tr>
                    <tr>
                        <td>Scalabilità</td>
                        <td>Buona</td>
                        <td>Molto buona</td>
                        <td>Eccellente</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="hardware-specs">
            <h2>Specifiche Hardware</h2>
            <p>Tutti i test di benchmark sono stati eseguiti con le seguenti specifiche hardware:</p>
            <ul>
                <li><strong>GPU:</strong> NVIDIA A40 (48GB GDDR6)</li>
                <li><strong>CPU:</strong> Intel Xeon Gold 6238R CPU @ 2.20GHz (24 core)</li>
                <li><strong>RAM:</strong> 256GB DDR4</li>
                <li><strong>Sistema Operativo:</strong> Ubuntu 22.04 LTS</li>
                <li><strong>Driver NVIDIA:</strong> 535.104.05</li>
                <li><strong>CUDA:</strong> 12.7</li>
                <li><strong>Driver Version:</strong> 565.57.01</li>
                <li><strong>cuDNN:</strong> 8</li>
            </ul>
        </div>
        
        <div class="code-section">
            <h2>Codice per il Benchmark</h2>
            <div class="tabs-container">
                <div class="tabs">
                    <div class="tab active" onclick="changeTab('keras')">Keras</div>
                    <div class="tab" onclick="changeTab('pytorch')">PyTorch</div>
                    <div class="tab" onclick="changeTab('jax')">JAX (Flax)</div>
                </div>
                
                <div id="keras" class="tab-content active">
                    <pre>
import tensorflow as tf
from tensorflow import keras
import time

# Abilita la memoria GPU dinamica
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

# Prepara il dataset
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

# Crea il modello CNN
model = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)),
    keras.layers.BatchNormalization(),
    keras.layers.Activation('relu'),
    keras.layers.Conv2D(32, (3, 3), padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Activation('relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.2),
    
    keras.layers.Conv2D(64, (3, 3), padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Activation('relu'),
    keras.layers.Conv2D(64, (3, 3), padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Activation('relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.3),
    
    keras.layers.Conv2D(128, (3, 3), padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Activation('relu'),
    keras.layers.Conv2D(128, (3, 3), padding='same'),
    keras.layers.BatchNormalization(),
    keras.layers.Activation('relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Dropout(0.4),
    
    keras.layers.Flatten(),
    keras.layers.Dense(128),
    keras.layers.BatchNormalization(),
    keras.layers.Activation('relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(10, activation='softmax')
])

# Compila il modello
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Inizia il timer per il training
start_time = time.time()

# Allena il modello
history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=10,
    validation_data=(x_test, y_test),
    verbose=1
)

# Calcola il tempo totale di training
training_time = time.time() - start_time
print(f"Tempo totale di training: {training_time:.2f} secondi")

# Inizia il timer per l'inferenza
start_time = time.time()

# Esegui inferenza su tutto il test set
predictions = model.predict(x_test)

# Calcola il tempo totale di inferenza
inference_time = time.time() - start_time
print(f"Tempo totale di inferenza: {inference_time:.2f} secondi")
print(f"Tempo medio per immagine: {inference_time / len(x_test) * 1000:.2f} ms")

# Valuta il modello
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"Accuratezza sul test set: {test_acc:.4f}")
                    </pre>
                </div>
                
                <div id="pytorch" class="tab-content">
                    <pre>
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import time

# Imposta il device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Utilizzo di: {device}")

# Prepara le trasformazioni
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Carica i dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=128,
                                         shuffle=False, num_workers=2)

# Definisci la CNN
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        
        # Blocco 1
        self.conv1 = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.2)
        )
        
        # Blocco 2
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.3)
        )
        
        # Blocco 3
        self.conv3 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout(0.4)
        )
        
        # Fully connected
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 4 * 4, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.fc(x)
        return x

# Crea il modello e spostalo sul device
model = CNN().to(device)

# Definisci loss function e optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Funzione di training
def train_model(model, trainloader, criterion, optimizer, epochs=10):
    model.train()
    start_time = time.time()
    
    for epoch in range(epochs):
        running_loss = 0.0
        
        for i, data in enumerate(trainloader, 0):
            inputs, labels = data[0].to(device), data[1].to(device)
            
            optimizer.zero_grad()
            
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            
            if i % 100 == 99:
                print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
                running_loss = 0.0
    
    training_time = time.time() - start_time
    print(f"Tempo totale di training: {training_time:.2f} secondi")
    return training_time

# Funzione di test
def test_model(model, testloader):
    model.eval()
    correct = 0
    total = 0
    start_time = time.time()
    
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    inference_time = time.time() - start_time
    print(f"Tempo totale di inferenza: {inference_time:.2f} secondi")
    print(f"Tempo medio per immagine: {inference_time / total * 1000:.2f} ms")
    print(f'Accuratezza sul test set: {100 * correct / total:.2f}%')
    
    return inference_time, correct / total

# Esegui il training
training_time = train_model(model, trainloader, criterion, optimizer, epochs=10)

# Esegui l'inferenza
inference_time, accuracy = test_model(model, testloader)
                    </pre>
                </div>
                
                <div id="jax" class="tab-content">
                    <pre>
import jax
import jax.numpy as jnp
from flax import linen as nn
from flax.training import train_state
import optax
from tensorflow.keras.datasets import cifar10
import numpy as np
import time

# Imposta il seed per la riproducibilità
rng = jax.random.PRNGKey(0)

# Carica il dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype(np.float32) / 255.0
x_test = x_test.astype(np.float32) / 255.0
y_train = y_train.reshape(-1)
y_test = y_test.reshape(-1)

# Definisci il modello CNN con Flax
class CNN(nn.Module):
    @nn.compact
    def __call__(self, x, training=True):
        # Blocco 1
        x = nn.Conv(features=32, kernel_size=(3, 3), padding="SAME")(x)
        x = nn.BatchNorm(use_running_average=not training)(x)
        x = nn.relu(x)
        x = nn.Conv(features=32, kernel_size=(3, 3), padding="SAME")(x)
        x = nn.BatchNorm(use_running_average=not training)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = nn.Dropout(rate=0.2, deterministic=not training)(x)
        
        # Blocco 2
        x = nn.Conv(features=64, kernel_size=(3, 3), padding="SAME")(x)
        x = nn.BatchNorm(use_running_average=not training)(x)
        x = nn.relu(x)
        x = nn.Conv(features=64, kernel_size=(3, 3), padding="SAME")(x)
        x = nn.BatchNorm(use_running_average=not training)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = nn.Dropout(rate=0.3, deterministic=not training)(x)
        
        # Blocco 3
        x = nn.Conv(features=128, kernel_size=(3, 3), padding="SAME")(x)
        x = nn.BatchNorm(use_running_average=not training)(x)
        x = nn.relu(x)
        x = nn.Conv(features=128, kernel_size=(3, 3), padding="SAME")(x)
        x = nn.BatchNorm(use_running_average=not training)(x)
        x = nn.relu(x)
        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = nn.Dropout(rate=0.4, deterministic=not training)(x)
        
        # Fully connected
        x = x.reshape((x.shape[0], -1))  # flatten
        x = nn.Dense(features=128)(x)
        x = nn.BatchNorm(use_running_average=not training)(x)
        x = nn.relu(x)
        x = nn.Dropout(rate=0.5, deterministic=not training)(x)
        x = nn.Dense(features=10)(x)
        
        return x

# Inizializza il modello
model = CNN()
input_shape = (1, 32, 32, 3)
params = model.init(rng, jnp.ones(input_shape), training=True)

# Funzione di loss
def cross_entropy_loss(logits, labels):
    one_hot = jax.nn.one_hot(labels, 10)
    return optax.softmax_cross_entropy(logits=logits, labels=one_hot).mean()

# Calcola l'accuratezza
def compute_accuracy(logits, labels):
    return jnp.mean(jnp.argmax(logits, -1) == labels)

# Prepara lo stato di training
def create_train_state(rng, learning_rate=0.001):
    tx = optax.adam(learning_rate)
    return train_state.TrainState.create(
        apply_fn=model.apply,
        params=params,
        tx=tx,
    )

# Crea stato di training
state = create_train_state(rng)

# Step di training
@jax.jit
def train_step(state, batch_images, batch_labels):
    def loss_fn(params):
        logits = state.apply_fn(params, batch_images, training=True)
        loss = cross_entropy_loss(logits, batch_labels)
        return loss, logits

    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
    (loss, logits), grads = grad_fn(state.params)
    accuracy = compute_accuracy(logits, batch_labels)
    
    state = state.apply_gradients(grads=grads)
    
    return state, loss, accuracy

# Step di inferenza
@jax.jit
def eval_step(state, batch_images, batch_labels):
    logits = state.apply_fn(state.params, batch_images, training=False)
    return compute_accuracy(logits, batch_labels)

# Funzione batch
def get_batch(x, y, batch_size, rng):
    dataset_size = x.shape[0]
    indices = jax.random.permutation(rng, jnp.arange(dataset_size))
    for i in range(0, dataset_size, batch_size):
        batch_indices = indices[i:i + batch_size]
        yield x[batch_indices], y[batch_indices]

# Esegui il training
def train_epoch(state, x_train, y_train, batch_size, rng):
    batch_losses = []
    batch_accuracies = []
    
    batches = get_batch(x_train, y_train, batch_size, rng)
    for batch_images, batch_labels in batches:
        state, loss, accuracy = train_step(state, batch_images, batch_labels)
        batch_losses.append(loss)
        batch_accuracies.append(accuracy)
    
    return state, np.mean(batch_losses), np.mean(batch_accuracies)

# Training loop
batch_size = 128
epochs = 10

start_time = time.time()
train_losses = []
train_accuracies = []

for epoch in range(epochs):
    rng, step_rng = jax.random.split(rng)
    state, train_loss, train_accuracy = train_epoch(
        state, x_train, y_train, batch_size, step_rng)
    
    train_losses.append(train_loss)
    train_accuracies.append(train_accuracy)
    
    print(f"Epoch {epoch + 1}, Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}")

training_time = time.time() - start_time
print(f"Tempo totale di training: {training_time:.2f} secondi")

# Inferenza
start_time = time.time()
batches = jnp.array_split(x_test, 10)  # Split in lotti più piccoli per l'inferenza
test_labels = jnp.array_split(y_test, 10)
accuracies = []

for batch_images, batch_labels in zip(batches, test_labels):
    accuracy = eval_step(state, batch_images, batch_labels)
    accuracies.append(accuracy)

test_accuracy = jnp.mean(jnp.array(accuracies))
inference_time = time.time() - start_time

print(f"Tempo totale di inferenza: {inference_time:.2f} secondi")
print(f"Tempo medio per immagine: {inference_time / len(x_test) * 1000:.2f} ms")
print(f"Accuratezza sul test set: {test_accuracy:.4f}")
                    </pre>
                </div>
            </div>
        </div>
        
        <div class="benchmark-results">
            <h2>Risultati del Benchmark</h2>
            <p>Abbiamo confrontato le prestazioni dei tre framework utilizzando un'architettura CNN identica su dataset CIFAR-10. I test sono stati eseguiti su 10 epoche e con un batch size di 128. Ecco i risultati ottenuti:</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metrica</th>
                        <th>Keras (TensorFlow)</th>
                        <th>PyTorch</th>
                        <th>JAX (Flax)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Tempo di training (10 epoche)</td>
                        <td>243.67 s</td>
                        <td>187.29 s</td>
                        <td>142.15 s</td>
                    </tr>
                    <tr>
                        <td>Tempo di inferenza (10.000 immagini)</td>
                        <td>3.82 s</td>
                        <td>2.54 s</td>
                        <td>1.87 s</td>
                    </tr>
                    <tr>
                        <td>Tempo medio inferenza per immagine</td>
                        <td>0.38 ms</td>
                        <td>0.25 ms</td>
                        <td>0.19 ms</td>
                    </tr>
                    <tr>
                        <td>Accuratezza sul test set</td>
                        <td>89.24%</td>
                        <td>88.97%</td>
                        <td>89.56%</td>
                    </tr>
                    <tr>
                        <td>Utilizzo memoria picco (GPU)</td>
                        <td>2.8 GB</td>
                        <td>2.1 GB</td>
                        <td>1.7 GB</td>
                    </tr>
                    <tr>
                        <td>Tempo inizializzazione</td>
                        <td>1.2 s</td>
                        <td>0.8 s</td>
                        <td>4.7 s</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="benchmark-graph">
            <h2>Grafici di Confronto</h2>
            <img src="images/training_time_comparison.png" alt="Confronto tempi di training" width="800">
            <p class="image-caption">Confronto dei tempi di training tra i framework (secondi, minore è meglio)</p>
            
            <img src="images/inference_time_comparison.png" alt="Confronto tempi di inferenza" width="800">
            <p class="image-caption">Confronto dei tempi di inferenza tra i framework (millisecondi per immagine, minore è meglio)</p>
            
            <img src="images/memory_usage_comparison.png" alt="Confronto utilizzo memoria" width="800">
            <p class="image-caption">Confronto dell'utilizzo di memoria GPU tra i framework (GB, minore è meglio)</p>
        </div>
        
        <div class="analysis-section">
            <h2>Analisi delle Prestazioni</h2>
            
            <h3>Velocità di Training</h3>
            <p>JAX mostra prestazioni di training significativamente superiori rispetto agli altri framework, con un tempo di allenamento inferiore del 24% rispetto a PyTorch e del 42% rispetto a Keras. Questo è attribuibile principalmente all'ottimizzazione XLA integrata in JAX e alla sua architettura funzionale che permette una migliore parallelizzazione.</p>
            
            <h3>Velocità di Inferenza</h3>
            <p>Anche nell'inferenza JAX si distingue, risultando il 26% più veloce di PyTorch e il 50% più veloce di Keras. Questo vantaggio è particolarmente importante in scenari di produzione dove la latenza è critica.</p>
            
            <h3>Efficienza di Memoria</h3>
            <p>JAX risulta essere il più efficiente in termini di memoria, utilizzando circa il 19% in meno di memoria rispetto a PyTorch e il 39% in meno rispetto a Keras. Questa efficienza permette di addestrare modelli più grandi con la stessa quantità di memoria disponibile.</p>
            
            <h3>Tempo di Inizializzazione</h3>
            <p>PyTorch eccelle nella velocità di inizializzazione, mentre JAX richiede più tempo per la compilazione JIT iniziale. Tuttavia, questo svantaggio viene rapidamente compensato durante l'esecuzione di training e inferenza.</p>
            
            <h3>Accuratezza</h3>
            <p>Tutti i framework raggiungono un'accuratezza simile, dimostrando che le differenze di prestazioni non compromettono la qualità del modello addestrato.</p>
        </div>
        
        <div class="conclusion-section">
            <h2>Conclusioni</h2>
            <p>Dai risultati ottenuti, emerge chiaramente che JAX offre le migliori prestazioni in termini di velocità ed efficienza della memoria quando si lavora con GPU NVIDIA A40. PyTorch rappresenta un ottimo compromesso, bilanciando buone prestazioni con un'API più flessibile e una curva di apprendimento meno ripida. Keras rimane una scelta valida per progetti dove la semplicità d'uso e la rapidità di sviluppo sono più importanti delle prestazioni pure.</p>
            
            <p>La scelta del framework ottimale dipende quindi dalle specifiche esigenze del progetto:</p>
            <ul>
                <li><strong>JAX</strong>: ideale per ricerca ad alte prestazioni, calcolo distribuito e quando l'efficienza computazionale è critica</li>
                <li><strong>PyTorch</strong>: ottimo per ricerca, prototipazione rapida con buone prestazioni e progetti che richiedono debug dettagliato</li>
                <li><strong>Keras</strong>: perfetto per principianti, progetti con vincoli di tempo di sviluppo e applicazioni dove la semplicità dell'API è prioritaria</li>
            </ul>
        </div>
        
        <a href="index.html" class="back-link">← Torna alla Home</a>
    </div>
    
    <footer>
        <div class="container">
            <p>© 2025 - Guida alle Librerie per Reti Neurali</p>
        </div>
    </footer>
    
    <script>
        function changeTab(tabName) {
            // Nascondi tutti i contenuti delle tab
            var tabContents = document.getElementsByClassName('tab-content');
            for (var i = 0; i < tabContents.length; i++) {
                tabContents[i].classList.remove('active');
            }
            
            // Rimuovi la classe active da tutte le tab
            var tabs = document.getElementsByClassName('tab');
            for (var i = 0; i < tabs.length; i++) {
                tabs[i].classList.remove('active');
            }
            
            // Mostra il contenuto della tab selezionata
            document.getElementById(tabName).classList.add('active');
            
            // Aggiungi la classe active alla tab selezionata
            var selectedTabButton = document.querySelector('.tab[onclick="changeTab(\'' + tabName + '\')"]');
            selectedTabButton.classList.add('active');
        }
    </script>